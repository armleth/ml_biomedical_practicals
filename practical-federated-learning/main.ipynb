{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ca055e-34a4-415d-8bc5-d6a6f2b9ff40",
   "metadata": {},
   "source": [
    "# Federated learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0c971-d415-40b6-a6fb-2ed697f156d2",
   "metadata": {},
   "source": [
    "First, we load a simple image dataset (like MNIST,  FashionMNIST) and we create a simple Convolutional Neural Network and check that your training works on a signle neural network (on a subset of the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d5eb0ca-5066-4c40-8b36-ee279a5afa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running sanity check ---\n",
      "Sanity check passed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 50\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "full_train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "def get_subset(dataset, num_samples, start_index=0):\n",
    "    indices = list(range(start_index, start_index + num_samples))\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = torch.relu(self.mp(self.conv1(x)))\n",
    "        x = torch.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "def sanity_check():\n",
    "    print(\"--- Running sanity check ---\")\n",
    "    model = SimpleCNN().to(DEVICE)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    subset = get_subset(full_train_dataset, 600)\n",
    "    loader = DataLoader(subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(5):\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = nn.functional.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(\"Sanity check passed.\\n\")\n",
    "\n",
    "sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cdb872-92f7-4981-b71e-73d05423f9c2",
   "metadata": {},
   "source": [
    "We then create a function average_model_parameters(models: iterable, average_weight): that average the parameters of each model parameters following the approach in the article. Afterward, we create a function that reproduces Algorithm 1 in the article. We consider that all local models are trained on your local machine and not remotly. We do not implement the common weight initialization scheme for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b6a906-ed17-4f5a-8804-084721dfda90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_model_parameters(models, weights):\n",
    "    with torch.no_grad():\n",
    "        avg_model = copy.deepcopy(models[0])\n",
    "        \n",
    "        for param in avg_model.parameters():\n",
    "            param.data.zero_()\n",
    "            \n",
    "        for i, model in enumerate(models):\n",
    "            weight = weights[i]\n",
    "            for avg_param, client_param in zip(avg_model.parameters(), model.parameters()):\n",
    "                avg_param.data.add_(client_param.data * weight)\n",
    "                \n",
    "        return avg_model\n",
    "\n",
    "def train_local_model(model, dataset, epochs=5):\n",
    "    model.train()\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = nn.functional.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4dc3d-80a6-46ab-a07d-60db03c8534d",
   "metadata": {},
   "source": [
    "We then run a training of two models with average coefficients being 0.5 for each model. Each model is trained on 600 data points each. We reuse the same setup as in the article (50 examples per local batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8e0c213-40fb-4aa5-be5f-3ee59b736bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running failure experiment ---\n",
      "Model A Accuracy: 42.95%\n",
      "Model B Accuracy: 55.73%\n",
      "Averaged Model Accuracy: 29.52%\n"
     ]
    }
   ],
   "source": [
    "def run_failure_experiment():\n",
    "    print(\"--- Running failure experiment ---\")\n",
    "    \n",
    "    model_A = SimpleCNN().to(DEVICE)\n",
    "    model_B = SimpleCNN().to(DEVICE)\n",
    "    \n",
    "    data_A = get_subset(full_train_dataset, 600, start_index=0)\n",
    "    data_B = get_subset(full_train_dataset, 600, start_index=600)\n",
    "    \n",
    "    model_A = train_local_model(model_A, data_A, epochs=10)\n",
    "    model_B = train_local_model(model_B, data_B, epochs=10)\n",
    "    \n",
    "    avg_model = average_model_parameters([model_A, model_B], [0.5, 0.5])\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000)\n",
    "    acc_A = evaluate_model(model_A, test_loader)\n",
    "    acc_B = evaluate_model(model_B, test_loader)\n",
    "    acc_Avg = evaluate_model(avg_model, test_loader)\n",
    "    \n",
    "    print(f\"Model A Accuracy: {acc_A:.2f}%\")\n",
    "    print(f\"Model B Accuracy: {acc_B:.2f}%\")\n",
    "    print(f\"Averaged Model Accuracy: {acc_Avg:.2f}%\")\n",
    "\n",
    "run_failure_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd16275-2ba0-4af5-88ea-51f3a08f49c5",
   "metadata": {},
   "source": [
    "We can see that this model accuray is very low: around 25%. The reason for that, as explained in the article, is that neural network loss functions are non-convex. Even if two models solve the same task, they will find different local minima. Because the order of neurons in hidden layers is arbitrary, \"Neuron 1\" in model A might represent a diagonal line detector, while \"Neuron 1\" in model B represents a circle detector. When you average these weights without a shared history, you destroy the internal structure of the neurons. The resulting averaged model lands in a high-loss region rather than a valid solution.\n",
    "\n",
    "We are then updating the training setup so that our models are initialized with of common set of  parameters. We then run a training in this setting and we make a study to see the impact of the number of data points on the performance of the combined model. We run training with 2, 3, 5 models, with each setting having : 25, 50, 100, 200 and 500 data points each. We then generate an array to represent accuracy of the global model with each model and datapoints number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e21125-4282-49f1-9a17-d3ddefaadce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running federated learning study ---\n",
      "# Models   | Data points          | Global model acc\n",
      "-------------------------------------------------------\n",
      "2          | 25                   | 19.49%\n",
      "2          | 50                   | 12.67%\n",
      "2          | 100                  | 27.21%\n",
      "2          | 200                  | 56.72%\n",
      "2          | 500                  | 84.49%\n",
      "3          | 25                   | 10.10%\n",
      "3          | 50                   | 18.53%\n",
      "3          | 100                  | 28.28%\n",
      "3          | 200                  | 62.49%\n",
      "3          | 500                  | 83.90%\n",
      "5          | 25                   | 25.02%\n",
      "5          | 50                   | 23.95%\n",
      "5          | 100                  | 41.49%\n",
      "5          | 200                  | 51.34%\n",
      "5          | 500                  | 82.66%\n"
     ]
    }
   ],
   "source": [
    "def run_federated_study():\n",
    "    print(\"\\n--- Running federated learning study ---\")\n",
    "    \n",
    "    num_models_list = [2, 3, 5]\n",
    "    data_points_list = [25, 50, 100, 200, 500]\n",
    "    results = {}\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000)\n",
    "\n",
    "    print(f\"{'# Models':<10} | {'Data points':<20} | {'Global model acc':<15}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    for n_models in num_models_list:\n",
    "        results[n_models] = {}\n",
    "        for n_data in data_points_list:\n",
    "            global_model = SimpleCNN().to(DEVICE)\n",
    "            \n",
    "            client_datasets = []\n",
    "            for i in range(n_models):\n",
    "                start_idx = i * n_data\n",
    "                client_datasets.append(get_subset(full_train_dataset, n_data, start_index=start_idx))\n",
    "            \n",
    "            for round_num in range(5):\n",
    "                local_models = []\n",
    "                \n",
    "                for ds in client_datasets:\n",
    "                    local_model = copy.deepcopy(global_model)\n",
    "                    local_model = train_local_model(local_model, ds, epochs=5)\n",
    "                    local_models.append(local_model)\n",
    "                \n",
    "                weights = [1.0/n_models] * n_models\n",
    "                global_model = average_model_parameters(local_models, weights)\n",
    "            \n",
    "            acc = evaluate_model(global_model, test_loader)\n",
    "            results[n_models][n_data] = acc\n",
    "            \n",
    "            print(f\"{n_models:<10} | {n_data:<20} | {acc:.2f}%\")\n",
    "\n",
    "    return results\n",
    "\n",
    "study_results = run_federated_study()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec41e7aa-81ff-4a8a-870d-c11c3e631d4b",
   "metadata": {},
   "source": [
    "We then repeat the study on another dataset like  HAM 10000:\n",
    "For this to work, you have to download `https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000` and to put it in `data/skin/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8db1753-227d-4acd-9712-f05a012bb81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Total number of images: 10015\n",
      "\n",
      "--- Running federated study ---\n",
      "Device: cpu\n",
      "\n",
      "# Models   | Data/Client     | Global Acc     \n",
      "---------------------------------------------\n",
      "2          | 50              | 67.80%\n",
      "2          | 200             | 67.80%\n",
      "2          | 500             | 67.80%\n",
      "3          | 50              | 67.80%\n",
      "3          | 200             | 67.80%\n",
      "3          | 500             | 67.80%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "IMG_DIR_1 = './data/skin/HAM10000_images_part_1'\n",
    "IMG_DIR_2 = './data/skin/HAM10000_images_part_2' \n",
    "CSV_PATH = './data/skin/HAM10000_metadata.csv'\n",
    "\n",
    "class HAM10000Dataset(Dataset):\n",
    "    def __init__(self, csv_file, dir1, dir2, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.dir1 = dir1\n",
    "        self.dir2 = dir2\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.label_map = {\n",
    "            'nv': 0, 'mel': 1, 'bkl': 2, 'bcc': 3, \n",
    "            'akiec': 4, 'vasc': 5, 'df': 6\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_filename = self.df.iloc[idx, 1] + \".jpg\"\n",
    "        label_str = self.df.iloc[idx, 2]\n",
    "        label = self.label_map[label_str]\n",
    "\n",
    "        path1 = os.path.join(self.dir1, img_filename)\n",
    "        path2 = os.path.join(self.dir2, img_filename)\n",
    "        \n",
    "        if os.path.exists(path1):\n",
    "            final_path = path1\n",
    "        elif os.path.exists(path2):\n",
    "            final_path = path2\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Image {img_filename} not found in either folder.\")\n",
    "\n",
    "        image = Image.open(final_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "ham_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.763, 0.546, 0.570], std=[0.141, 0.153, 0.170]) \n",
    "])\n",
    "\n",
    "try:\n",
    "    full_dataset = HAM10000Dataset(\n",
    "        csv_file=CSV_PATH, \n",
    "        dir1=IMG_DIR_1, \n",
    "        dir2=IMG_DIR_2, \n",
    "        transform=ham_transform\n",
    "    )\n",
    "    print(f\"Dataset loaded. Total number of images: {len(full_dataset)}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    print(\"Ensure you have unzipped both 'part_1' and 'part_2' folders into ./data/skin/\")\n",
    "    exit()\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "\n",
    "class SimpleCNN_HAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN_HAM, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5) \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5)\n",
    "        \n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(800, 120)\n",
    "        self.fc2 = nn.Linear(120, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = torch.relu(self.mp(self.conv1(x)))\n",
    "        x = torch.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1) # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def train_local_model(model, dataset, epochs=5):\n",
    "    model.train()\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = nn.functional.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "def average_model_parameters(models, weights):\n",
    "    with torch.no_grad():\n",
    "        avg_model = copy.deepcopy(models[0])\n",
    "        for param in avg_model.parameters():\n",
    "            param.data.zero_()\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            w = weights[i]\n",
    "            for avg_p, client_p in zip(avg_model.parameters(), model.parameters()):\n",
    "                avg_p.data.add_(client_p.data * w)\n",
    "    return avg_model\n",
    "\n",
    "def evaluate_model(model, test_dataset):\n",
    "    model.eval()\n",
    "    loader = DataLoader(test_dataset, batch_size=100)\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    return 100. * correct / len(loader.dataset)\n",
    "\n",
    "def get_subset_indices(dataset, num_samples, start_index=0):\n",
    "    return Subset(dataset, range(start_index, start_index + num_samples))\n",
    "\n",
    "\n",
    "\n",
    "def run_ham10000_study():\n",
    "    print(f\"\\n--- Running federated study ---\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    \n",
    "    num_models_list = [2, 3]\n",
    "    data_points_list = [50, 200, 500]\n",
    "    \n",
    "    print(f\"\\n{'# Models':<10} | {'Data points':<15} | {'Global ccc':<15}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for n_models in num_models_list:\n",
    "        for n_data in data_points_list:\n",
    "            global_model = SimpleCNN_HAM().to(DEVICE)\n",
    "            \n",
    "            client_datasets = []\n",
    "            valid_setup = True\n",
    "            for i in range(n_models):\n",
    "                start_idx = i * n_data\n",
    "                if start_idx + n_data > len(train_dataset):\n",
    "                    print(f\"Not enough data for {n_models} models with {n_data} samples.\")\n",
    "                    valid_setup = False\n",
    "                    break\n",
    "                client_datasets.append(get_subset_indices(train_dataset, n_data, start_idx))\n",
    "            \n",
    "            if not valid_setup: continue\n",
    "\n",
    "            for round_num in range(3):\n",
    "                local_models = []\n",
    "                \n",
    "                for ds in client_datasets:\n",
    "                    local_model = copy.deepcopy(global_model)\n",
    "                    local_model = train_local_model(local_model, ds, epochs=3)\n",
    "                    local_models.append(local_model)\n",
    "                \n",
    "                weights = [1.0/n_models] * n_models\n",
    "                global_model = average_model_parameters(local_models, weights)\n",
    "            \n",
    "            acc = evaluate_model(global_model, test_dataset)\n",
    "            print(f\"{n_models:<10} | {n_data:<15} | {acc:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_ham10000_study()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
